{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import dataset\n",
    "import pandas as pd\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import pickle\n",
    "#https://github.com/toxtli/lenet-5-mnist-from-scratch-numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC():\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        #print(\"Build FC\")\n",
    "        self.cache = None\n",
    "        #self.W = {'val': np.random.randn(D_in, D_out), 'grad': 0}\n",
    "        self.W = {'val': np.random.normal(0.0, np.sqrt(2/D_in), (D_in,D_out)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(D_out), 'grad': 0}\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"FC: _forward\")\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"FC: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "        #self._update_params()\n",
    "        return dX\n",
    "\n",
    "    def _update_params(self, lr=0.001):\n",
    "        # Update the parameters\n",
    "        self.W['val'] -= lr*self.W['grad']\n",
    "        self.b['val'] -= lr*self.b['grad']\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build ReLU\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"ReLU: _forward\")\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"ReLU: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Softmax activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build Softmax\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"Softmax: _forward\")\n",
    "        maxes = np.amax(X, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        Y = np.exp(X - maxes)\n",
    "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "        self.cache = (X, Y, Z)\n",
    "        return Z # distribution\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, Y, Z = self.cache\n",
    "        dZ = np.zeros(X.shape)\n",
    "        dY = np.zeros(X.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        N = X.shape[0]\n",
    "        for n in range(N):\n",
    "            i = np.argmax(Z[n])\n",
    "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
    "            M = np.zeros((N,N))\n",
    "            M[:,i] = 1\n",
    "            dY[n,:] = np.eye(N) - M\n",
    "        dX = np.dot(dout,dZ)\n",
    "        dX = np.dot(dX,dY)\n",
    "        return dX\n",
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])\n",
    "class Net(metaclass=ABCMeta):\n",
    "    # Neural network super class\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_params(self, params):\n",
    "        pass\n",
    "class NN(Net):\n",
    "\n",
    "    def __init__(self,in_,mid_,weights=''):\n",
    "\n",
    "        self.FC1 = FC(in_, mid_)\n",
    "        self.FC2 = FC(mid_, mid_)\n",
    "        self.FC3=FC(mid_, 2)\n",
    "        self.relu=ReLU()\n",
    "        self.Softmax = Softmax()\n",
    "        if weights == '':\n",
    "            pass\n",
    "        else:\n",
    "            with open(weights,'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "                self.set_params(params)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x=self.FC1._forward(x)\n",
    "        x=self.relu._forward(x)\n",
    "        x= self.FC2._forward(x)\n",
    "        x=self.relu._forward(x)\n",
    "        x= self.FC3._forward(x)\n",
    "        x= self.Softmax._forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.relu._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.relu._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b,self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b,self.FC3.W, self.FC3.b] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeOneHot(Y, D_out=2):\n",
    "    N = Y.shape[0]\n",
    "    Z = np.zeros((N, D_out))\n",
    "    Z[np.arange(N), Y] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##這裡要改路徑\n",
    "data=dataset('C:\\\\Users\\\\Robert\\\\Desktop\\\\hw2\\\\train.csv')\n",
    "data.train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model,select_feature,epoch=20,data=data.train_data,label=data.train_label):\n",
    "    optim = SGD(model.get_params())\n",
    "    for i in range(epoch):\n",
    "        for index, (row,labels) in enumerate(zip(data,label)):\n",
    "            row=np.expand_dims(row,axis=0)\n",
    "            labels=np.expand_dims(labels,axis=0)\n",
    "            labels=MakeOneHot(labels,2)\n",
    "            Y_pred = model.forward(row[:,select_feature])\n",
    "            #先算到softmax的back\n",
    "            dout=Y_pred-labels\n",
    "            model.backward(dout)\n",
    "            optim.step()\n",
    "    acc=0\n",
    "    for index, (row,labels) in enumerate(zip(data,label)):\n",
    "        row=np.expand_dims(row,axis=0)\n",
    "        Y_pred = model.forward(row[:,select_feature])\n",
    "        Y_pred=np.argmax(Y_pred,1)\n",
    "        if Y_pred[0]==labels:\n",
    "            acc+=1\n",
    "    print(f\"Accuracy: {acc/(len(label))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model,select_feature,data=data,label=data):\n",
    "    acc=0\n",
    "    for index, (row,labels) in enumerate(zip(data,label)):\n",
    "        row=np.expand_dims(row,axis=0)\n",
    "        Y_pred = model.forward(row[:,select_feature])\n",
    "        Y_pred=np.argmax(Y_pred,1)\n",
    "        if Y_pred[0]==labels:\n",
    "            acc+=1\n",
    "    print(f\"Accuracy: {acc/(len(label))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "numbers = list(range(1, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948\n",
      "Accuracy: 0.948\n",
      "Accuracy: 0.948\n",
      "Accuracy: 0.948\n",
      "Accuracy: 0.948\n"
     ]
    }
   ],
   "source": [
    "nn1=NN(10,20)\n",
    "selected_1 = random.sample(numbers, 10)\n",
    "model_train(nn1,selected_1)\n",
    "\n",
    "\n",
    "nn2=NN(10,20)\n",
    "selected_2 = random.sample(numbers, 10)\n",
    "model_train(nn2,selected_2)\n",
    "\n",
    "nn3=NN(10,20)\n",
    "selected_3 = random.sample(numbers, 10)\n",
    "model_train(nn1,selected_3)\n",
    "\n",
    "nn4=NN(10,20)\n",
    "selected_4 = random.sample(numbers, 10)\n",
    "model_train(nn1,selected_4)\n",
    "\n",
    "nn5=NN(10,20)\n",
    "selected_5 = random.sample(numbers, 10)\n",
    "model_train(nn1,selected_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9380440348182284\n",
      "Accuracy: 0.9380440348182284\n",
      "Accuracy: 0.061955965181771634\n",
      "Accuracy: 0.6033452807646356\n",
      "Accuracy: 0.844342037890425\n"
     ]
    }
   ],
   "source": [
    "model_test(nn1,selected_1,data.val_data,data.val_label)\n",
    "model_test(nn2,selected_2,data.val_data,data.val_label)\n",
    "model_test(nn3,selected_3,data.val_data,data.val_label)\n",
    "model_test(nn4,selected_4,data.val_data,data.val_label)\n",
    "model_test(nn5,selected_5,data.val_data,data.val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9368600682593856\n",
      "Accuracy: 0.9368600682593856\n",
      "Accuracy: 0.06313993174061433\n",
      "Accuracy: 0.606655290102389\n",
      "Accuracy: 0.8435153583617747\n"
     ]
    }
   ],
   "source": [
    "model_test(nn1,selected_1,data.test_data,data.test_label)\n",
    "model_test(nn2,selected_2,data.test_data,data.test_label)\n",
    "model_test(nn3,selected_3,data.test_data,data.test_label)\n",
    "model_test(nn4,selected_4,data.test_data,data.test_label)\n",
    "model_test(nn5,selected_5,data.test_data,data.test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_test(model1,model2,model3,model4,model5,selected_1,selected_2,selected_3,selected_4,selected_5,data,label):\n",
    "    acc=0\n",
    "    for index, (row,labels) in enumerate(zip(data,label)):\n",
    "        Y_pred=[]\n",
    "        for i in range(1,6):\n",
    "            row_i=np.expand_dims(row,axis=0)\n",
    "            model='model'+str(i)\n",
    "            select='selected_'+str(i)\n",
    "            pred = eval(model).forward(row_i[:,eval(select)])\n",
    "            Y_pred.append(np.argmax(pred,1)[0])\n",
    "        Y_pred=int(sum(Y_pred)/len(Y_pred))\n",
    "        if Y_pred==labels:\n",
    "            acc+=1\n",
    "    print(f\"Accuracy: {acc/(len(label))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948\n",
      "Accuracy: 0.9380440348182284\n",
      "Accuracy: 0.9368600682593856\n"
     ]
    }
   ],
   "source": [
    "combine_test(nn1,nn2,nn3,nn4,nn5,selected_1,selected_2,selected_3,selected_4,selected_5,data.train_data,data.train_label)\n",
    "combine_test(nn1,nn2,nn3,nn4,nn5,selected_1,selected_2,selected_3,selected_4,selected_5,data.val_data,data.val_label)\n",
    "combine_test(nn1,nn2,nn3,nn4,nn5,selected_1,selected_2,selected_3,selected_4,selected_5,data.test_data,data.test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
